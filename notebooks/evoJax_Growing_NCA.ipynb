{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Growing Neural Cellular automata using Evolutionary Strategies\n",
        "\n",
        "In this notebook we train the [growing NCA](https://distill.pub/2020/growing-ca/) using evolutionary strategies and the [evoJax](https://github.com/google/evojax) library.\n",
        "\n",
        "\n",
        "An extensive explanation of the problem and the CA model can be found in [this article](https://distill.pub/2020/growing-ca/), the current notebook will be mainly used for explaining the training with evoJax."
      ],
      "metadata": {
        "id": "XK0DCIB27rOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evojax"
      ],
      "metadata": {
        "id": "Pkw55F-b5o8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx28vZvw7iOs"
      },
      "outputs": [],
      "source": [
        "# @title Importing libraries\n",
        "\n",
        "import os\n",
        "import io\n",
        "import PIL\n",
        "import PIL.Image, PIL.ImageDraw, PIL.ImageFont, PIL.ImageOps\n",
        "import base64\n",
        "import matplotlib.pylab as pl\n",
        "import numpy as np\n",
        "from IPython.display import display\n",
        "from functools import partial\n",
        "from IPython.display import Image, HTML, clear_output\n",
        "import matplotlib.animation as animation\n",
        "import time\n",
        "\n",
        "import einops\n",
        "import requests\n",
        "\n",
        "\n",
        "\n",
        "#jax\n",
        "import jax\n",
        "import jax.numpy as jp\n",
        "from jax import grad, jit, vmap, pmap\n",
        "from jax.example_libraries import optimizers\n",
        "import jax.random as jr\n",
        "from jax.flatten_util import ravel_pytree\n",
        "import flax.linen as fnn\n",
        "from jax.sharding import PartitionSpec as P\n",
        "\n",
        "\n",
        "\n",
        "#evojax\n",
        "import evojax\n",
        "from evojax.algo import PGPE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Imports and Notebook Utilities\n",
        "os.environ['FFMPEG_BINARY'] = 'ffmpeg'\n",
        "clear_output()\n",
        "\n",
        "def np2pil(a):\n",
        "  if a.dtype in [np.float32, np.float64]:\n",
        "    a = np.uint8(np.clip(a, 0, 1)*255)\n",
        "  return PIL.Image.fromarray(a)\n",
        "\n",
        "def imwrite(f, a, fmt=None):\n",
        "  a = np.asarray(a)\n",
        "  if isinstance(f, str):\n",
        "    fmt = f.rsplit('.', 1)[-1].lower()\n",
        "    if fmt == 'jpg':\n",
        "      fmt = 'jpeg'\n",
        "    f = open(f, 'wb')\n",
        "  np2pil(a).save(f, fmt, quality=95)\n",
        "\n",
        "def imencode(a, fmt='jpeg'):\n",
        "  a = np.asarray(a)\n",
        "  if len(a.shape) == 3 and a.shape[-1] == 4:\n",
        "    fmt = 'png'\n",
        "  f = io.BytesIO()\n",
        "  imwrite(f, a, fmt)\n",
        "  return f.getvalue()\n",
        "\n",
        "def im2url(a, fmt='jpeg'):\n",
        "  encoded = imencode(a, fmt)\n",
        "  base64_byte_string = base64.b64encode(encoded).decode('ascii')\n",
        "  return 'data:image/' + fmt.upper() + ';base64,' + base64_byte_string\n",
        "\n",
        "\n",
        "def imshow(a, fmt='jpeg', display_id=None):\n",
        "  display(Image(data=imencode(a, fmt)), display_id=display_id)\n",
        "\n",
        "def tile2d(a, w=None):\n",
        "  a = np.asarray(a)\n",
        "  if w is None:\n",
        "    w = int(np.ceil(np.sqrt(len(a))))\n",
        "  th, tw = a.shape[1:3]\n",
        "  pad = (w-len(a))%w\n",
        "  a = np.pad(a, [(0, pad)]+[(0, 0)]*(a.ndim-1), 'constant')\n",
        "  h = len(a)//w\n",
        "  a = a.reshape([h, w]+list(a.shape[1:]))\n",
        "  a = np.rollaxis(a, 2, 1).reshape([th*h, tw*w]+list(a.shape[4:]))\n",
        "  return a\n",
        "\n",
        "def zoom(img, scale=4):\n",
        "  img = np.repeat(img, scale, 0)\n",
        "  img = np.repeat(img, scale, 1)\n",
        "\n",
        "  return img\n",
        "\n",
        "class VideoWriter:\n",
        "  def __init__(self, filename, fps=30.0, **kw):\n",
        "    self.writer = None\n",
        "    self.params = dict(filename=filename, fps=fps, **kw)\n",
        "\n",
        "  def add(self, img):\n",
        "    img = np.asarray(img)\n",
        "    if self.writer is None:\n",
        "      h, w = img.shape[:2]\n",
        "      self.writer = FFMPEG_VideoWriter(size=(w, h), **self.params)\n",
        "    if img.dtype in [np.float32, np.float64]:\n",
        "      img = np.uint8(img.clip(0, 1)*255)\n",
        "    if len(img.shape) == 2:\n",
        "      img = np.repeat(img[..., None], 3, -1)\n",
        "    self.writer.write_frame(img)\n",
        "\n",
        "  def close(self):\n",
        "    if self.writer:\n",
        "      self.writer.close()\n",
        "\n",
        "  def __enter__(self):\n",
        "    return self\n",
        "\n",
        "  def __exit__(self, *kw):\n",
        "    self.close()"
      ],
      "metadata": {
        "id": "hW70xKQM8pBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cellular Automata Parameters\n",
        "CHANNEL_N = 16        # Number of CA state channels\n",
        "TARGET_PADDING = 4   # Number of pixels used to pad the target image border\n",
        "TARGET_SIZE = 40\n",
        "BATCH_SIZE = 1\n",
        "CELL_FIRE_RATE = 0.5\n",
        "\n",
        "TARGET_EMOJI = \"ðŸ¦Ž\""
      ],
      "metadata": {
        "id": "5xuxAEdu-i9w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title CA Model and Utilities\n",
        "\n",
        "def load_image(url, max_size=TARGET_SIZE):\n",
        "  r = requests.get(url)\n",
        "  img = PIL.Image.open(io.BytesIO(r.content))\n",
        "  img.thumbnail((max_size, max_size), PIL.Image.LANCZOS)\n",
        "  img = np.float32(img)/255.0\n",
        "  # premultiply RGB by Alpha\n",
        "  img[..., :3] *= img[..., 3:]\n",
        "  return img\n",
        "\n",
        "def load_emoji(emoji):\n",
        "  code = hex(ord(emoji))[2:].lower()\n",
        "  url = 'https://github.com/googlefonts/noto-emoji/blob/main/png/128/emoji_u%s.png?raw=true'%code\n",
        "  return load_image(url)\n",
        "\n",
        "\n",
        "def visualize(frames, namefile, size, negate=0):\n",
        "  # Create and save animation\n",
        "  fig, ax = pl.subplots(figsize=(size, size))\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "\n",
        "  def animate(frame):\n",
        "    ax.clear()\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "\n",
        "    if negate:\n",
        "      frame = 1 - frame\n",
        "\n",
        "    frame = np.clip(frame, 0, 1)\n",
        "\n",
        "    return [ax.imshow(frame)]\n",
        "\n",
        "  anim = animation.FuncAnimation(fig, animate, frames=frames, interval=200, blit=True)\n",
        "\n",
        "  # Save the gif\n",
        "  writer = animation.PillowWriter(fps=10)\n",
        "  anim.save(namefile, writer=writer)\n",
        "  pl.close()\n",
        "\n",
        "\n",
        "def to_rgba(x):\n",
        "  return x[..., :4]\n",
        "\n",
        "def to_alpha(x):\n",
        "  return x[..., 3:4].clip(0.0, 1.0)\n",
        "\n",
        "def to_rgb(x):\n",
        "  # assume rgb premultiplied by alpha\n",
        "  rgb, a = x[..., :3], to_alpha(x)\n",
        "  return 1.0-a+rgb\n",
        "\n",
        "def get_living_mask(x):\n",
        "  # probably not needed anymore.\n",
        "  alpha = x[:, :, :, 3:4]\n",
        "  return fnn.max_pool(alpha, window_shape=(3,3), strides=(1, 1),\n",
        "                      padding='SAME') \u003e 0.1\n",
        "\n",
        "def make_seed(size, n=1):\n",
        "  x = np.zeros([n, size, size, CHANNEL_N], np.float32)\n",
        "  x[:, size//2, size//2, 3:] = 1.0\n",
        "  return x\n",
        "\n",
        "def depthwise_conv2d(x, kernel, strides, padding):\n",
        "  c = x.shape[-1]\n",
        "  x = einops.rearrange(x, 'b h w c -\u003e (b c) () h w')\n",
        "  y = jax.lax.conv(x, kernel, strides, padding)\n",
        "  y = einops.rearrange(y, '(b c) f h w -\u003e b h w (f c)', c=c)\n",
        "  return y\n",
        "\n",
        "def perceive(x, angle=0.0):\n",
        "  identify = np.float32([0, 1, 0])\n",
        "  identify = np.outer(identify, identify)\n",
        "  dx = np.outer([1, 2, 1], [-1, 0, 1]) / 8.0  # Sobel filter\n",
        "  dy = dx.T\n",
        "  c, s = jp.cos(angle), jp.sin(angle)\n",
        "  kernel = jp.stack([identify, c*dx-s*dy, s*dx+c*dy], 0)[:,None, :, :]\n",
        "  y = depthwise_conv2d(x, kernel, (1, 1), 'SAME')\n",
        "  return y\n",
        "\n",
        "\n",
        "class CellsUpdate(fnn.Module):\n",
        "  channel_n: int = CHANNEL_N\n",
        "  fire_rate: float = CELL_FIRE_RATE\n",
        "\n",
        "  def setup(self):\n",
        "    \"\"\"Initializes all parameters.\n",
        "\n",
        "    \"\"\"\n",
        "    self.dmodel = fnn.Sequential([\n",
        "        fnn.Conv(features=128, kernel_size=(1,1)),\n",
        "        fnn.relu,\n",
        "        fnn.Conv(features=self.channel_n, kernel_size=(1,1),\n",
        "                 kernel_init=jax.nn.initializers.zeros)\n",
        "    ])\n",
        "\n",
        "  def __call__(self, x, key, fire_rate=None, angle=0.0, step_size=1.0):\n",
        "    pre_life_mask = get_living_mask(x)\n",
        "\n",
        "    y = perceive(x, angle)\n",
        "    dx = self.dmodel(y)*step_size\n",
        "    if fire_rate is None:\n",
        "      fire_rate = self.fire_rate\n",
        "    update_mask_f32 = (jr.uniform(key, x[:, :, :, :1].shape) \u003c= fire_rate).astype(jp.float32)\n",
        "    x += dx * update_mask_f32\n",
        "\n",
        "    post_life_mask = get_living_mask(x)\n",
        "    life_mask_f32 = (pre_life_mask \u0026 post_life_mask).astype(jp.float32)\n",
        "    return x * life_mask_f32"
      ],
      "metadata": {
        "id": "ldpSvRVMB19m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def visualize_batch(x0, x, step_i):\n",
        "  zoomed_x0 = zoom(to_rgb(x0), 2)\n",
        "  zoomed_x = zoom(to_rgb(x), 2)\n",
        "  vis0 = np.hstack(zoomed_x0[::2])\n",
        "  vis1 = np.hstack(zoomed_x[::2])\n",
        "  vis = np.vstack([vis0, vis1])\n",
        "  imshow(vis)\n",
        "\n",
        "def plot_loss(loss_log):\n",
        "  pl.figure(figsize=(10, 4))\n",
        "  pl.title('Loss history (log10)')\n",
        "  pl.plot(np.log10(-np.asarray(loss_log)), '.', alpha=0.1)\n",
        "  pl.show()\n"
      ],
      "metadata": {
        "id": "Iod9VOBiCBI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load emoji\n",
        "\n",
        "target_img = load_emoji(TARGET_EMOJI)\n",
        "imshow(zoom(to_rgb(target_img), 2), fmt='png')"
      ],
      "metadata": {
        "id": "-2Kkd20YCFon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialization of the training\n",
        "\n",
        "Evolutionary Strategies (ES) typically don't require schedulers used in gradient methods.\n",
        "However, ES requires proper parameter formatting according to the specific library's (e.g., evoJax) requirements.\n",
        "\n",
        "Key Concepts for ES Optimizers:\n",
        "1. **Population**: The population is a set of parameters tested during each ES iteration. For a model with 10 parameters and a population size of 100, the population has a dimension of (100, 10).\n",
        "Each element in the population represents a set of parameters to be evaluated.\n",
        "2. **Fitness Function**: Unlike gradient descent, ES aims to maximize a fitness function. Each population element is evaluated using this function.\n",
        "3. **Solver Algorithm**: The population elements with the best fitness are used to create the next population. The specific algorithm determines how this happens. This Colab will explore [PGPE ](https://people.idsia.ch/~juergen/icann2008sehnke.pdf), which mimics gradient descent. Many other options exist.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The ES-optimizer defined by the evoJAX exhibits 3 main API methods:\n",
        "\n",
        "1. **Initializer** in which the type of algorithm is specified with all the hyperparameters (like the size of the population and the dimension of each population element)\n",
        "1. **solver.ask()**: It provides a population of parameter sets. The output is a vector with the shape (population size, parameter size). This necessitates flattening the parameters before applying evolutionary strategies.\n",
        "2. **solver.tell(fitness)**: Once the population elements are obtained and their fitnesses are evaluated, we need to provide these fitness values to the solver which update its own parameters. This is done using the solver.tell(fitness) function. **Important**: The fitness vector should have the shape (population size, 1), where each element corresponds to the fitness of the respective population element.\n",
        "\n",
        "\n",
        "To initialize training, we defined the fitness function *fitness_f*. To use evolutionary strategies, we must flatten the parameters (for the reason explained above). The *ravel_pytree* function is used for this, and *unravel_pytree* is used within the fitness function to reconstruct the original parameter shapes."
      ],
      "metadata": {
        "id": "TLcPXghFCqbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize Training { vertical-output: true}\n",
        "\n",
        "p = TARGET_PADDING\n",
        "pad_target = jp.pad(target_img, [(p, p), (p, p), (0, 0)])\n",
        "h, w = pad_target.shape[:2]\n",
        "seed = np.zeros([h, w, CHANNEL_N], np.float32)\n",
        "seed[h//2, w//2, 3:] = 1.0\n",
        "x0 = np.repeat(seed[None, ...], BATCH_SIZE, 0)\n",
        "\n",
        "def target_loss_f(x):\n",
        "  return (jp.square(to_rgba(x)-pad_target)).mean([-2, -3, -1])\n",
        "\n",
        "\n",
        "#Initialization of the cell model\n",
        "ca_update = CellsUpdate()\n",
        "key = jr.PRNGKey(1)\n",
        "k1, k2, key = jr.split(key, 3)\n",
        "\n",
        "\n",
        "params = ca_update.init(k1, seed[None,:], k2)\n",
        "\n",
        "#flatten all the parameters\n",
        "params_flat, unravel_pytree = ravel_pytree(params)\n",
        "\n",
        "\n",
        "@jit\n",
        "def fitness_f(params_flat, x, key):\n",
        "\n",
        "  iter_n = 90\n",
        "  compute_loss_every=30\n",
        "  params = unravel_pytree(params_flat)\n",
        "\n",
        "  loss = 0.0\n",
        "\n",
        "  def scan_f(carry, i):\n",
        "    loss, x, key = carry\n",
        "    key, keyused = jr.split(key)\n",
        "    x = ca_update.apply(params, x, key)\n",
        "\n",
        "    loss = jax.lax.cond(((i+1)%compute_loss_every==0), (lambda loss, x: loss + target_loss_f(x).mean()), (lambda loss, x: loss), loss, x)\n",
        "\n",
        "    return (loss, x, key), 0\n",
        "\n",
        "    #loss function computed with all the timesteps from 0 to iter_n\n",
        "\n",
        "  (loss, x, key), _ = jax.lax.scan(\n",
        "      scan_f, (loss, x, key), jp.arange(0, iter_n, dtype=jp.int32))\n",
        "\n",
        "  return -loss, x\n",
        "\n",
        "k1, key = jr.split(key, 2)\n",
        "imshow(zoom(to_rgb(pad_target), 2), fmt='png')"
      ],
      "metadata": {
        "id": "nUubcMpUCKkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#parallelized using pmap\n",
        "\n",
        "#only used if Training Device is TPU\n",
        "def p_fitness_fn(ca_params, x0, key):\n",
        "  #each device then execute the vectorized version of the fitness function (vmap)\n",
        "  fitness, x = pmap(v_fitness_fn)(ca_params, x0, key)\n",
        "  return fitness, x\n",
        "\n",
        "\n",
        "@jit\n",
        "def v_fitness_fn(ca_params, x0, key):\n",
        "  fitness, x = vmap(fitness_f)(ca_params, x0, key)\n",
        "  return fitness, x\n"
      ],
      "metadata": {
        "id": "PVwkBgOJKVbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If TPU is the device used for training, then we split the population in equal chunks, each chunk will be allocated to a specific XLA device to increase performance. This approach demonstrated strong linear scaling (for the same problem size, doubling the computational resources reduces the computational time by half) and constant weak scaling (doubling both the problem size and computational resources â€”XLA devicesâ€” maintains approximately constant computational time)."
      ],
      "metadata": {
        "id": "SIWB26IdSpiu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters of the optimizer\n",
        "\n",
        "# For popsize=128\n",
        "lr=0.08\n",
        "\n",
        "# For popsize=256\n",
        "#lr=0.1\n",
        "\n",
        "# param_size is the total number of parameters in the flattened parameter vector.\n",
        "param_size=len(params_flat)\n",
        "popsize= 128\n",
        "\n",
        "optimizer='adam'\n",
        "center_learning_rate=0.05*lr\n",
        "stdev_learning_rate=0.01*lr\n",
        "init_stdev=0.1*lr\n",
        "\n",
        "training_iterations = 50000\n",
        "print_every=100\n",
        "\n",
        "\n",
        "if 'tpu' in [device.platform for device in jax.devices()]:\n",
        "  #select TPU for training\n",
        "  TPU = 8\n",
        "\n",
        "  # Initialize the TPU mesh for memory sharding.\n",
        "  # This distributes the population across XLA devices to improve performance.\n",
        "  mesh = jax.make_mesh((TPU,), ('x'))\n",
        "  sharding = jax.sharding.NamedSharding(mesh, P('x'))\n",
        "\n",
        "\n",
        "  @jax.jit\n",
        "  def repeat_for_tpu(tensor):\n",
        "      \"\"\"\n",
        "      Repeat tensor from (batch, x, y, channel) to (tpu, pop_size/tpu, x, y, channel)\n",
        "      \"\"\"\n",
        "      expanded = jax.numpy.expand_dims(tensor, axis=0)  # (1, batch, x, y, channel)\n",
        "      tiled = jax.numpy.repeat(expanded, popsize, axis=0)  # (tpu, batch, x, y, channel)\n",
        "      reshaped = tiled.reshape(TPU, popsize//TPU, tensor.shape[0], tensor.shape[1], tensor.shape[2], tensor.shape[3])\n",
        "      return reshaped\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def repeat_for_gpu(tensor):\n",
        "    \"\"\"\n",
        "    Repeat tensor from (batch, x, y, channel) to (tpu, pop_size/tpu, x, y, channel)\n",
        "    \"\"\"\n",
        "    expanded = jax.numpy.expand_dims(tensor, axis=0)  # (1, batch, x, y, channel)\n",
        "    tiled = jax.numpy.repeat(expanded, popsize, axis=0)  # (popsize, batch, x, y, channel)\n",
        "\n",
        "    return tiled\n",
        "\n",
        "\n",
        "#Initializer of the solver\n",
        "solver = PGPE(\n",
        "    pop_size=popsize,\n",
        "    param_size=param_size,\n",
        "    optimizer=optimizer,\n",
        "    center_learning_rate=center_learning_rate,\n",
        "    stdev_learning_rate=stdev_learning_rate,\n",
        "    seed=1,\n",
        "    init_stdev=init_stdev,\n",
        ")\n"
      ],
      "metadata": {
        "id": "uyUz82i6I1cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training Loop\n",
        "\n",
        "#init variables\n",
        "time_ = 0.0\n",
        "time_step = 0.0\n",
        "i = 0\n",
        "\n",
        "start_time = time.time()\n",
        "best_fitness = []\n",
        "\n",
        "x0 = np.repeat(seed[None, ...], BATCH_SIZE, 0)\n",
        "\n",
        "if 'tpu' in [device.platform for device in jax.devices()]:\n",
        "  x0_tiled = repeat_for_tpu(x0)\n",
        "else:\n",
        "  x0_tiled = repeat_for_gpu(x0)\n",
        "\n",
        "\n",
        "while i\u003ctraining_iterations+1:\n",
        "\n",
        "  k1, key = jr.split(key)\n",
        "\n",
        "  #get the parameters from the solver (new populatiopn)\n",
        "  v_params = solver.ask()\n",
        "\n",
        "  #reshape the parameters for allowing parallel execution on different devices using pmap\n",
        "  key, skey = jr.split(key)\n",
        "  skey_tiled = jr.split(skey, popsize)\n",
        "\n",
        "  #TPU\n",
        "  if 'tpu' in [device.platform for device in jax.devices()]:\n",
        "    #reshape the parameters for allowing parallel execution on different devices using pmap\n",
        "    pop_per_tpu = popsize // TPU\n",
        "    v_params = jax.numpy.reshape(v_params, (TPU, pop_per_tpu,-1))\n",
        "    v_params = jax.device_put(v_params, sharding)\n",
        "    skey_tiled = jax.numpy.reshape(skey_tiled, (TPU, pop_per_tpu,-1))\n",
        "    v_params = jax.numpy.reshape(v_params, (TPU, pop_per_tpu, -1))\n",
        "\n",
        "    #compute the fitness of the parameters\n",
        "    fitness, x = p_fitness_fn(v_params, x0_tiled, skey_tiled)\n",
        "    fitness = einops.rearrange(fitness, 't p -\u003e (t p)')\n",
        "\n",
        "\n",
        "  #GPU/CPU\n",
        "  else:\n",
        "    fitness, x = v_fitness_fn(v_params, x0_tiled, skey_tiled)\n",
        "\n",
        "  best_fitness.append(float(fitness.max().item()))\n",
        "\n",
        "  #update the solver\n",
        "  solver.tell(fitness)\n",
        "\n",
        "  if(i%print_every == 0):\n",
        "      if 'tpu' in [device.platform for device in jax.devices()]:\n",
        "        x = einops.rearrange(x, 't p b w h c -\u003e (t p) b w h c')\n",
        "      best_fitness_arg = jp.argmax(fitness)\n",
        "      clear_output()\n",
        "      best_fitness_arg = jp.argmax(fitness)\n",
        "      visualize_batch(x0, x[best_fitness_arg], i)\n",
        "      plot_loss(best_fitness)\n",
        "\n",
        "  time_ = time.time() - start_time\n",
        "  time_step = time_/(i+1)\n",
        "\n",
        "  print('\\r popsize: %d, step: %d, fitness: %.4f , loss: %.4f , total_time: %.2f , time_step: %.3f'%(popsize, i, best_fitness[-1], -best_fitness[-1],  time_, time_step), end='')\n",
        "\n",
        "  i+=1\n",
        "\n"
      ],
      "metadata": {
        "id": "dZyMEZ3SIxpu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params = solver.ask()[0]\n",
        "params = unravel_pytree(params)\n",
        "x0 = np.repeat(seed[None, ...], BATCH_SIZE, 0)\n",
        "x = x0\n",
        "frames = []\n",
        "for _ in range(100):\n",
        "  k1, key = jr.split(k1, 2)\n",
        "  x = ca_update.apply(params, x, key)\n",
        "  frames.append(zoom(to_rgb(x)[0],4))\n",
        "\n",
        "visualize(frames, 'lizard.gif', 10)\n",
        "Image('lizard.gif')"
      ],
      "metadata": {
        "id": "Y-zNrrV6RHqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
